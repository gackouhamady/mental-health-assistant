{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e81d193",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img alt=\"University Paris Cité\" src=\"https://img.shields.io/badge/University-Paris%20Cité-6f42c1?style=for-the-badge&logo=academia&logoColor=white\">\n",
    "  <img alt=\"Master ML for Data Science\" src=\"https://img.shields.io/badge/Master-Machine%20Learning%20for%20Data%20Science-1976D2?style=for-the-badge&logo=python&logoColor=white\">\n",
    "  <img alt=\"Project LLMs\" src=\"https://img.shields.io/badge/Project-Mental%20Health%20Assistant-FF9800?style=for-the-badge&logo=transformers&logoColor=white\">\n",
    "  <img alt=\"Academic Year\" src=\"https://img.shields.io/badge/Year-2025%2F-009688?style=for-the-badge&logo=googlecalendar&logoColor=white\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "  <strong>Master 2 — Machine Learning for Data Science</strong><br>\n",
    "  <strong>Project : Mental Health Assistant (Empathy-driven Dialogue Generation)</strong><br>\n",
    "  <strong>Author : Hamady GACKOU</strong>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "##  Project Information  \n",
    "\n",
    "| **Category**        | **Details**                                                                 |\n",
    "|---------------------|------------------------------------------------------------------------------|\n",
    "| **University**      | Université Paris Cité                                                        |\n",
    "| **Master Program**  | Machine Learning for Data Science (MLDS/AMSD)                                |\n",
    "| **Course**          | Natural Language Processing & Large Language Models                          |\n",
    "| **Student**         | Hamady GACKOU                                                                |\n",
    "| **Dataset**         | [EmpatheticDialogues](https://huggingface.co/datasets/facebook/empathetic_dialogues) |\n",
    "| **Objective**       | Develop a prototype of a Mental Health Assistant using pre-trained LLMs (DistilGPT-2) to generate empathetic responses |\n",
    "| **Keywords**        | NLP • Empathy • LLM • Hugging Face • Transformers • Dialogue Systems         |\n",
    "| **Year**   | 2025                                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1152db",
   "metadata": {},
   "source": [
    "##  Objectif du Projet\n",
    "Développer un assistant virtuel capable de générer des réponses empathiques et utiles en matière de bien-être mental, sans se substituer à un professionnel de santé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e178005",
   "metadata": {},
   "source": [
    "##  Phase 1 – Lancement & Exploration\n",
    "- Définir les objectifs précis du projet  \n",
    "- Comparer les modèles (GPT-2, DistilGPT-2, BERT, T5)  \n",
    "- Créer un tableau comparatif (capacité, efficacité CPU, complexité)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad633f6",
   "metadata": {},
   "source": [
    "###  Comparatif des Modèles Candidats\n",
    "\n",
    "| Modèle | Type | Paramètres | Taille approx. | Points forts | Limites | Pertinence pour l'empathie |\n",
    "|:--|:--:|:--:|:--:|:--|:--|:--|\n",
    "| **GPT-2 (small)** | Decoder | 124 M | ~500 MB | Bonne fluidité, capable de dialogues longs | Lent sur CPU, consomme beaucoup de RAM | ⭐⭐⭐ |\n",
    "| **DistilGPT-2** | Decoder distillé | 82 M | ~300 MB | Léger, rapide sur CPU, bon équilibre qualité-coût | Moins précis sur les émotions complexes | ⭐⭐⭐⭐ |\n",
    "| **BERT (base-uncased)** | Encoder | 110 M | ~400 MB | Excellente compréhension du texte | Non génératif (classification uniquement) | ⭐ |\n",
    "| **T5-small** | Encoder-Decoder | 60 M | ~240 MB | Génératif et multitâche | Sorties parfois mécaniques sur peu de données | ⭐⭐⭐ |\n",
    "\n",
    "---\n",
    "\n",
    "###  Interprétation\n",
    "- Les **modèles “Decoder only”** (GPT-2 et DistilGPT-2) sont naturellement meilleurs pour **la génération empathique**.  \n",
    "- **DistilGPT-2** est **le plus adapté pour ton projet** :  \n",
    "  - Compatible **CPU**, rapide à l’inférence,  \n",
    "  - Suffisamment cohérent pour des réponses naturelles,  \n",
    "  - Facile à fine-tuner sur des données empathiques comme *EmpatheticDialogues*.  \n",
    "- **T5-small** est un bon candidat secondaire si tu veux explorer les architectures **encoder-decoder**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Choix final pour la suite du projet\n",
    " **Modèle retenu : `DistilGPT-2`**  \n",
    "Justification : compromis optimal entre **vitesse**, **cohérence**, **efficacité CPU** et **capacité à produire des réponses empathiques** après fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1d894",
   "metadata": {},
   "source": [
    "##  Phase 2 – Préparation des Données\n",
    "\n",
    "###  Objectif\n",
    "Charger et préparer le dataset **EmpatheticDialogues** de Facebook AI afin d’obtenir des données propres et prêtes pour le fine-tuning.\n",
    "\n",
    "###  Étapes\n",
    "1. **Chargement du dataset** depuis Hugging Face  \n",
    "2. **Nettoyage** des textes : suppression des doublons, normalisation et retrait des caractères spéciaux  \n",
    "3. **Analyse exploratoire rapide** : longueur moyenne des dialogues, distribution des émotions  \n",
    "4. **Tokenisation** pour le modèle DistilGPT-2  \n",
    "\n",
    "---\n",
    "\n",
    "### Dataset\n",
    " Nom : `facebook/empathetic_dialogues`  \n",
    " Taille : ~25 000 dialogues (train / valid / test)  \n",
    " Contenu : contextes + réponses humaines à 32 types d’émotions (e.g., sad, proud, jealous, grateful, anxious, afraid…)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37246ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataFilesPatternsDict' from 'datasets.data_files' (/home/researcherdatascientist/mental-health-assistant/venv/lib/python3.11/site-packages/datasets/data_files.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChargement du dataset EmpatheticDialogues...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mental-health-assistant/venv/lib/python3.11/site-packages/datasets/__init__.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concatenate_datasets, interleave_datasets\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict, IterableDatasetDict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mental-health-assistant/venv/lib/python3.11/site-packages/datasets/builder.py:51\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     HF_GCP_BASE_URL,\n\u001b[32m     45\u001b[39m     ArrowReader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     ReadInstruction,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, BeamWriter, ParquetWriter, SchemaInferenceError\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_files\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFilesDict, DataFilesPatternsDict, sanitize_patterns\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict, IterableDatasetDict\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'DataFilesPatternsDict' from 'datasets.data_files' (/home/researcherdatascientist/mental-health-assistant/venv/lib/python3.11/site-packages/datasets/data_files.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Chargement du dataset EmpatheticDialogues...\")\n",
    "dataset = load_dataset(\"facebook/empathetic_dialogues\", trust_remote_code=True)\n",
    "\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "print(\"Taille du jeu d'entraînement :\", len(train_df))\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0542e",
   "metadata": {},
   "source": [
    "##  Phase 3 – Prototype Simple (Baseline)\n",
    "- Création d’un **pipeline Hugging Face** pour la génération  \n",
    "- Test de génération avec **DistilGPT-2** non fine-tuné  \n",
    "- Validation rapide sur quelques exemples (\"I feel sad\", \"I feel anxious\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e0a32",
   "metadata": {},
   "source": [
    "##  Phase 4 – Fine-Tuning sur CPU\n",
    "- Entraînement rapide avec `Trainer` (DistilGPT-2)  \n",
    "- Paramètres : batch réduit + gradient accumulation  \n",
    "- Évaluation et sauvegarde du modèle fine-tuné\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcb906",
   "metadata": {},
   "source": [
    "##  Phase 5 – Ajustements & Ré-Entraînement\n",
    "- Réglage des hyperparamètres (learning rate, époques, batch size)  \n",
    "- Évaluation qualitative : empathie, cohérence, fluidité  \n",
    "- Option : utiliser **DistillBERT** pour la détection d’émotions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1010b5e",
   "metadata": {},
   "source": [
    "##  Phase 6 – Interface Utilisateur (Gradio)\n",
    "- Conception d’une interface simple et rassurante  \n",
    "- Mise en place avec **Gradio** (`Interface`, `Blocks`)  \n",
    "- Tests locaux et ajustements de la tonalité des réponses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8227a",
   "metadata": {},
   "source": [
    "##  Phase 7 – Tests & Validation Utilisateur\n",
    "- Panel de testeurs (pairs, étudiants, amis)  \n",
    "- Questionnaire d’évaluation : empathie, clarté, utilité  \n",
    "- Collecte et synthèse des retours pour amélioration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc87b3",
   "metadata": {},
   "source": [
    "##  Phase 8 – Finitions & Lancement\n",
    "- Nettoyage du code et amélioration finale de l’UI  \n",
    "- Rédaction d’un **README** clair et d’une note éthique  \n",
    "- Déploiement sur **Hugging Face Spaces**  \n",
    "- Publication sur **GitHub** et **LinkedIn**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf2ae1",
   "metadata": {},
   "source": [
    "##  Annexe – Tableau Comparatif des Modèles\n",
    "| Modèle | Paramètres | Taille | Avantage | Limite |\n",
    "|:--|:--:|:--:|:--|:--|\n",
    "| GPT-2 | 124M | ~500 MB | Bonne génération, stable | Lent sur CPU |\n",
    "| DistilGPT-2 | 82M | ~300 MB | Léger, rapide, bon compromis | Moins cohérent sur longs dialogues |\n",
    "| BERT | 110M | ~400 MB | Bonne compréhension de texte | Pas génératif |\n",
    "| T5-small | 60M | ~240 MB | Génératif et flexible | Plus lent que DistilGPT-2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84eb43",
   "metadata": {},
   "source": [
    "##  Résumé Final\n",
    "Un prototype **fonctionnel sur CPU**, avec :\n",
    "- Dataset empathique  \n",
    "- Modèle fine-tuné DistilGPT-2  \n",
    "- Interface Gradio conviviale  \n",
    "- Documentation éthique et technique prête pour déploiement\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
